# SentinelCLIP: Accountable Vision-Language Classification

**A production-ready framework for trustworthy image classification using CLIP with human-in-the-loop decision making, rigorous uncertainty quantification, and adaptive policy optimization.**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Code style: modular](https://img.shields.io/badge/code%20style-modular-brightgreen.svg)](https://github.com/megatron-iitb/SentinelCLIP)

---

## ğŸ¯ Overview

SentinelCLIP implements an **accountable AI system** that intelligently decides when predictions are trustworthy enough for autonomous execution versus when human oversight is needed. Built on CLIP's vision-language foundation, it combines state-of-the-art calibration techniques with explainable decision policies.

### Key Capabilities

- ğŸ¨ **Zero-shot Classification** via CLIP ViT-B-32
- ğŸ“Š **Multi-stage Calibration** (Temperature Scaling, Isotonic Regression, Conformal Prediction)
- ğŸ” **Semantic Reasoning** through natural language questions
- ğŸ”„ **Ensemble Uncertainty** via test-time augmentations
- ğŸ¤– **Adaptive Policy** with threshold optimization
- ğŸ“ˆ **Comprehensive Evaluation** with reliability diagrams and audit trails

---

## ğŸŒŸ Features

### Scientific Rigor

- **Calibrated Probabilities**: Temperature scaling with automatic grid-search fallback
- **Per-question Calibration**: Isotonic regression for semantic prompts
- **Conformal Prediction**: Finite-sample coverage guarantees (1-Î± confidence)
- **Ensemble Uncertainty**: Both standard deviation and entropy-based measures
- **Explainable Decisions**: Per-sample audit logs with rationale

### Engineering Excellence

- **Modular Architecture**: Clean pipeline stages (data â†’ calibration â†’ uncertainty â†’ policy â†’ evaluation)
- **Offline Support**: Pre-download models for HPC clusters
- **SLURM Integration**: Production-ready job scripts
- **Reproducibility**: Seeded RNG, version-controlled configs
- **Comprehensive Docs**: Architecture guides, API references, troubleshooting

### Three-Tier Decision System

```
High Confidence â†’ AUTO-EXECUTE    (autonomous prediction)
       â†“
Medium Confidence â†’ CLARIFY        (use ensemble + conformal sets)
       â†“
Low Confidence â†’ DEFER TO HUMAN    (request expert judgment)
```

---

## ğŸ“Š Performance

**Benchmark: CIFAR-10 with CLIP ViT-B-32**

| Metric | Value | Notes |
|--------|-------|-------|
| **Baseline Accuracy** | 86.16% | Zero-shot CLIP |
| **Post-Policy Accuracy** | 99.99% | With simulated human |
| **ECE (Calibrated)** | 0.042 | Well-calibrated probabilities |
| **Conformal Set Size** | 1.11 | Near-singleton prediction sets |
| **Intervention Recall** | 100% | All errors caught |
| **Runtime** | ~3.5 min | L40 GPU, 10k test samples |

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone git@github.com:megatron-iitb/SentinelCLIP.git
cd SentinelCLIP

# Create conda environment
conda create -n sentinelclip python=3.9
conda activate sentinelclip

# Install dependencies
pip install torch torchvision open-clip-torch
pip install scikit-learn matplotlib tqdm numpy
```

### Download Models (for offline use)

```bash
# Download CLIP model to cache
python3 -c "
import open_clip
model, _, _ = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')
print('âœ“ Model cached')
"

# CIFAR-10 will auto-download on first run
```

### Run Experiment

```bash
# Local execution
python run_experiment_modular.py

# SLURM cluster
sbatch job_experiment_1.sh

# Monitor progress
tail -f logs/Exp_1_modular.*.log
```

### Expected Output

```
======================================================================
EXPERIMENT 1: ACCOUNTABLE CLIP-BASED CLASSIFICATION
======================================================================

STEP 1-3: MODEL & DATA PREPARATION
âœ“ CLIP model loaded successfully
âœ“ Dataset loaded: 10 classes
âœ“ Val embeddings: (5000, 512)
âœ“ Test embeddings: (10000, 512)

STEP 4-8: CALIBRATION PIPELINE
âœ“ Baseline zero-shot test accuracy: 0.8616
âœ“ Temperature scaling: T=1.23
âœ“ Calibrated 9/9 questions
âœ“ Conformal threshold: 0.24 (90% coverage)

STEP 9-11: UNCERTAINTY ESTIMATION
âœ“ Ensemble confidence: mean=0.94
âœ“ Entropy-based confidence: mean=0.94
âœ“ OOD confidence: mean=0.85

STEP 12-14: POLICY & DECISION MAKING
âœ“ Optimized thresholds: tau=0.35, auto=0.80, clarify=0.65
âœ“ Decisions: auto=45%, clarify=35%, human=20%

STEP 15-17: EVALUATION & REPORTING
âœ“ Post-policy accuracy: 0.98
âœ“ Intervention precision: 0.42, recall: 0.95
âœ“ All outputs saved

âœ“ Experiment completed successfully!
```

---

## ğŸ“ Repository Structure

```
SentinelCLIP/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.py              # Central configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ clip_model.py      # CLIP wrapper
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ dataset.py         # Data loading & augmentation
â”‚   â”œâ”€â”€ calibration/
â”‚   â”‚   â”œâ”€â”€ temperature.py     # Temperature scaling
â”‚   â”‚   â”œâ”€â”€ isotonic.py        # Isotonic regression
â”‚   â”‚   â””â”€â”€ conformal.py       # Conformal prediction
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ questions.py       # Semantic question engine
â”‚   â”‚   â”œâ”€â”€ ensemble.py        # Augmentation ensemble
â”‚   â”‚   â””â”€â”€ metrics.py         # Evaluation metrics
â”‚   â”œâ”€â”€ policy/
â”‚   â”‚   â”œâ”€â”€ decision_policy.py # Accountable policy
â”‚   â”‚   â””â”€â”€ threshold_optimizer.py # Grid-search optimization
â”‚   â”œâ”€â”€ pipeline/              # High-level pipeline stages
â”‚   â”‚   â”œâ”€â”€ data_preparation.py
â”‚   â”‚   â”œâ”€â”€ calibration.py
â”‚   â”‚   â”œâ”€â”€ uncertainty.py
â”‚   â”‚   â”œâ”€â”€ policy.py
â”‚   â”‚   â””â”€â”€ evaluation.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ math_utils.py      # Numerical utilities
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md        # System design
â”‚   â”œâ”€â”€ LOG_ANALYSIS.md        # Result interpretation
â”‚   â”œâ”€â”€ ERROR_GUIDE.md         # Troubleshooting
â”‚   â”œâ”€â”€ QUICK_REFERENCE.md     # Command cheatsheet
â”‚   â””â”€â”€ MODULAR_IMPLEMENTATION.md # Refactoring notes
â”œâ”€â”€ logs/                      # Experiment logs
â”œâ”€â”€ outputs/                   # Results, plots, audit logs
â”œâ”€â”€ run_experiment_modular.py  # Main entry point
â”œâ”€â”€ job_experiment_1.sh        # SLURM job script
â””â”€â”€ README.md                  # This file
```

---

## âš™ï¸ Configuration

Edit `configs/config.py` to customize:

### Model & Data
```python
MODEL_NAME = "ViT-B-32"
MODEL_PRETRAINED = "openai"
DATASET_NAME = "CIFAR10"
BATCH_SIZE = 256
```

### Calibration
```python
CONFORMAL_ALPHA = 0.10          # 90% coverage target
TEMPERATURE_MAX_EPOCHS = 500
QUESTION_PROMPTS = [...]        # Semantic questions
```

### Policy
```python
SIM_HUMAN_ACCURACY = 1.0        # Simulated human accuracy
OPTIMIZE_THRESHOLDS = True      # Auto-tune thresholds
THRESHOLD_COST_HUMAN = 1.0      # Cost of human intervention
THRESHOLD_COST_ERROR = 10.0     # Cost of incorrect prediction
```

### Ensemble
```python
N_AUGMENTATIONS = 16
ENSEMBLE_CONF_STRATEGY = "combined"  # "std"|"entropy"|"combined"|"geometric"
```

---

## ğŸ”¬ Scientific Background

### Calibration Pipeline

1. **Temperature Scaling** ([Guo et al., 2017](https://arxiv.org/abs/1706.04599))
   - Rescales logits to improve probability calibration
   - Uses log-parameterization for numerical stability
   - Automatic grid-search fallback if optimizer hits bounds

2. **Isotonic Regression** ([Zadrozny & Elkan, 2002](https://dl.acm.org/doi/10.1145/502512.502570))
   - Non-parametric calibration per semantic question
   - Preserves monotonicity while correcting over-confidence
   - Fitted on validation data

3. **Conformal Prediction** ([Angelopoulos & Bates, 2021](https://arxiv.org/abs/2107.07511))
   - Provides prediction sets with finite-sample coverage guarantees
   - For Î±=0.10, ensures â‰¥90% coverage on test data
   - Near-singleton sets for high-confidence predictions

### Uncertainty Quantification

- **Std-based Ensemble**: Measures variation in predicted class across augmentations
- **Entropy-based**: Mutual information proxy: H(pred) - E[H(pred|aug)]
- **OOD Detection**: Uses prediction entropy to identify out-of-distribution samples

### Policy Optimization

Grid-searches thresholds (tau_critical_low, action_auto, action_clarify) to minimize:

```
cost = c_human Ã— (% interventions) + c_error Ã— (% errors)
```

Configurable cost weights allow tuning the precision-recall tradeoff.

---

## ğŸ“ˆ Usage Examples

### Basic Usage

```python
from src.pipeline import (
    setup_model_and_data,
    run_calibration_pipeline,
    run_uncertainty_estimation,
    run_policy_pipeline,
    run_evaluation
)
from configs import config
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Run full pipeline
data = setup_model_and_data(config, device)
calib = run_calibration_pipeline(data, config, device)
uncertainty = run_uncertainty_estimation(data, calib, config)
policy_results = run_policy_pipeline(data, calib, uncertainty, config, device)
metrics = run_evaluation(data, calib, uncertainty, policy_results, config)

print(f"Final accuracy: {metrics['post_acc']:.4f}")
```

### Custom Calibration

```python
from src.calibration.temperature import fit_temperature
from src.utils.math_utils import softmax

# Fit temperature on your data
temperature = fit_temperature(val_logits, val_labels, device='cuda')

# Apply to test set
test_probs_calibrated = softmax(test_logits / temperature, axis=1)
```

### Using the Policy

```python
from src.policy.decision_policy import AccountablePolicy

policy = AccountablePolicy(
    tau_critical_low=0.3,
    action_auto=0.8,
    action_clarify=0.65,
    sim_human_accuracy=0.95
)

decisions, final_preds = policy.make_decisions(
    action_confidence, min_crit, base_preds, true_labels,
    conformal_sets, set_sizes, ensemble_probs
)

print(f"Auto: {(decisions=='auto').sum()}, "
      f"Clarify: {(decisions=='clarify').sum()}, "
      f"Human: {(decisions=='human').sum()}")
```

---

## ğŸ”§ Advanced Usage

### Multi-Dataset Evaluation

```python
# In configs/config.py
DATASET_NAME = "CIFAR100"  # or "ImageNet", custom dataset

# Implement custom loader in src/data/dataset.py
class CustomDataset(torch.utils.data.Dataset):
    # Your implementation
    pass
```

### Adding New Calibration Methods

```python
# Create src/calibration/beta_calibration.py
def fit_beta_calibration(logits, labels):
    # Your implementation
    return calibrated_probs

# Import in src/pipeline/calibration.py
from src.calibration.beta_calibration import fit_beta_calibration

# Use in pipeline
beta_probs = fit_beta_calibration(val_logits, val_labels)
```

### Custom Question Prompts

```python
# In configs/config.py
QUESTION_PROMPTS = [
    "Is this object man-made?",
    "Is this a living organism?",
    "Does it have wheels?",
    # ... your domain-specific questions
]

QUESTION_GT_MAP = {
    "Is this object man-made?": [1, 8, 9],  # class indices
    # ... mappings for each question
}
```

---

## ğŸ“Š Evaluation Metrics

### Calibration Metrics

- **Expected Calibration Error (ECE)**: Measures reliability of probability estimates
- **Reliability Diagrams**: Visualize calibration across confidence bins
- **Conformal Set Size**: Efficiency of uncertainty sets

### Policy Metrics

- **Intervention Precision**: P(error | intervened)
- **Intervention Recall**: P(intervened | error)
- **Coverage**: Fraction of samples in each decision tier
- **Cost**: Weighted sum of interventions and errors

### Outputs

1. **Plots**: `outputs/reliability_diagram.png`, `outputs/coverage_vs_error.png`
2. **Audit Log**: `outputs/audit_log_test.jsonl` (per-sample decisions in JSONL format)
3. **Logs**: `logs/Exp_1_modular.*.log` (detailed execution trace)

---

## ğŸ› Troubleshooting

### Common Issues

**Temperature hits bounds (T=0.01 or T=100)**
- Expected behavior for well/poorly-calibrated models
- Grid-search fallback activates automatically
- See [ERROR_GUIDE.md](docs/ERROR_GUIDE.md#error-6-temperature-hits-bounds)

**All decisions = 'human' (no automation)**
- Root cause: Low question confidences or overly conservative thresholds
- Solution: Adjust `THRESHOLD_COST_ERROR` or `QUESTION_GT_MAP`
- See [LOG_ANALYSIS.md](docs/LOG_ANALYSIS.md#8-policy-threshold-optimization)

**Model download fails (offline mode)**
- Pre-download on login node before SLURM job
- See [docs/RUN_OFFLINE.md](docs/RUN_OFFLINE.md)

**CUDA out of memory**
- Reduce `BATCH_SIZE` or `N_AUGMENTATIONS`
- See [ERROR_GUIDE.md](docs/ERROR_GUIDE.md#error-3-runtimeerror---cuda-out-of-memory)

For more issues, see [docs/ERROR_GUIDE.md](docs/ERROR_GUIDE.md)

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [ARCHITECTURE.md](docs/ARCHITECTURE.md) | System design, module descriptions, data flow |
| [LOG_ANALYSIS.md](docs/LOG_ANALYSIS.md) | Result interpretation, diagnostics |
| [ERROR_GUIDE.md](docs/ERROR_GUIDE.md) | Common errors and solutions |
| [QUICK_REFERENCE.md](docs/QUICK_REFERENCE.md) | Commands, configs, troubleshooting checklist |
| [MODULAR_IMPLEMENTATION.md](docs/MODULAR_IMPLEMENTATION.md) | Refactoring notes, benchmarks |

---

## ğŸ§ª Testing

```bash
# Validate imports
python -c "from src.pipeline import *; print('âœ“ All imports OK')"

# Quick test (subset of data)
# Edit configs/config.py: VAL_SPLIT = 0.01, TEST_SIZE = 100
python run_experiment_modular.py

# Full test
pytest tests/  # (TODO: add unit tests)
```

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

1. **Unit tests** for all modules
2. **Additional datasets** (ImageNet, domain-specific)
3. **New calibration methods** (Platt scaling, beta calibration)
4. **Real human-in-the-loop interface** (web UI)
5. **Distributed training** support

Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

## ğŸ“– Citation

If you use SentinelCLIP in your research, please cite:

```bibtex
@software{sentinelclip2025,
  title={SentinelCLIP: Accountable Vision-Language Classification with Human-in-the-Loop},
  author={Rawat, Anupam},
  year={2025},
  institution={IIT Bombay},
  url={https://github.com/megatron-iitb/SentinelCLIP}
}
```

### References

- **CLIP**: Radford et al. (2021) "Learning Transferable Visual Models From Natural Language Supervision"
- **Temperature Scaling**: Guo et al. (2017) "On Calibration of Modern Neural Networks"
- **Conformal Prediction**: Angelopoulos & Bates (2021) "A Gentle Introduction to Conformal Prediction"
- **Isotonic Regression**: Zadrozny & Elkan (2002) "Transforming Classifier Scores into Accurate Multiclass Probability Estimates"

---

## ğŸ™ Acknowledgments

- **OpenAI** for CLIP models and vision-language research
- **OpenCLIP** team for the open-source implementation
- **IIT Bombay MEDAL Lab** for computational resources
- **CIFAR-10** dataset creators

---

## ğŸ‘¥ Team

**Maintainer**: Anupam Rawat (anupam.rawat@iitb.ac.in)  
**Institution**: IIT Bombay, MEDAL Lab  
**Advisor**: [Add advisor name]

---

## ğŸ“ Contact & Support

- **Issues**: [GitHub Issues](https://github.com/megatron-iitb/SentinelCLIP/issues)
- **Email**: anupam.rawat@iitb.ac.in
- **Documentation**: [docs/](docs/)

---

**Last Updated**: November 24, 2025  
**Version**: 2.0 (Modular Architecture)  
**Status**: Production Ready âœ…
